local Lexer = require("src.lexer").Lexer
local TokenType = require("src.token_type").TokenType

describe("This is another context", function()
    local TEST_NEXT_TOKEN = function(text, token_type)

    end
    before(function()
        TEST_NEXT_TOKEN = function(text, token_type)
            local lexer = Lexer:new({ text = text })
            local token = lexer:nextToken()
            assert_equal(token.type, token_type)
        end
    end)

    it("this is a auxiliary", function()
        TEST_NEXT_TOKEN('(', TokenType.LPAREN)
        TEST_NEXT_TOKEN(')', TokenType.RPAREN)
        TEST_NEXT_TOKEN(':', TokenType.COLON)
        TEST_NEXT_TOKEN('-', TokenType.ID)
        TEST_NEXT_TOKEN('+', TokenType.ID)
        TEST_NEXT_TOKEN('#', TokenType.SHARP)
        TEST_NEXT_TOKEN([[]], TokenType.EOF)
        TEST_NEXT_TOKEN([[']], TokenType.SINGLE_QUOTE)
    end)

    it("this is a identifier", function()
        TEST_NEXT_TOKEN('*a*', TokenType.ID)
        TEST_NEXT_TOKEN('a', TokenType.ID)
        TEST_NEXT_TOKEN('a-b-c', TokenType.ID)
        TEST_NEXT_TOKEN([["abc"]], TokenType.STRING)
        TEST_NEXT_TOKEN([[#\a]], TokenType.CHARACTER)
    end)

    it("this is a keyword", function()
        TEST_NEXT_TOKEN([[defclass]], TokenType.DEFCLASS)
        TEST_NEXT_TOKEN([[defconstant]], TokenType.DEFCONSTANT)
        TEST_NEXT_TOKEN([[defgeneric]], TokenType.DEFGENERIC)
        TEST_NEXT_TOKEN([[defmethod]], TokenType.DEFMETHOD)
        TEST_NEXT_TOKEN([[defparameter]], TokenType.DEFPARAMETER)
        TEST_NEXT_TOKEN([[defun]], TokenType.DEFUN)
        TEST_NEXT_TOKEN([[defvar]], TokenType.DEFVAR)
        TEST_NEXT_TOKEN([[do]], TokenType.DO)
        TEST_NEXT_TOKEN([[dolist]], TokenType.DOLIST)
        TEST_NEXT_TOKEN([[dotimes]], TokenType.DOTIMES)
        TEST_NEXT_TOKEN([[if]], TokenType.IF)
        TEST_NEXT_TOKEN([[for]], TokenType.FOR)
        TEST_NEXT_TOKEN([[in]], TokenType.IN)
        TEST_NEXT_TOKEN([[collect]], TokenType.COLLECT)
        TEST_NEXT_TOKEN([[map]], TokenType.MAP)
        TEST_NEXT_TOKEN([[lambda]], TokenType.LAMBDA)
        TEST_NEXT_TOKEN([[let]], TokenType.LET)
        TEST_NEXT_TOKEN([[loop]], TokenType.LOOP)
        TEST_NEXT_TOKEN([[when]], TokenType.WHEN)
        TEST_NEXT_TOKEN([[AND]], TokenType.AND)
        TEST_NEXT_TOKEN([[T]], TokenType.T)
        TEST_NEXT_TOKEN([[NIL]], TokenType.NIL)
    end)

    it("this is a number", function()
        TEST_NEXT_TOKEN('0', TokenType.INTEGER)
        TEST_NEXT_TOKEN('-0', TokenType.INTEGER)
        TEST_NEXT_TOKEN('+0', TokenType.INTEGER)
        TEST_NEXT_TOKEN('0.0', TokenType.FLOAT)
        TEST_NEXT_TOKEN('0/1', TokenType.INTEGER)
        TEST_NEXT_TOKEN('-1', TokenType.INTEGER)
        TEST_NEXT_TOKEN('+1', TokenType.INTEGER)
        TEST_NEXT_TOKEN('-1.2', TokenType.FLOAT)
        TEST_NEXT_TOKEN('+1.2', TokenType.FLOAT)
        TEST_NEXT_TOKEN('+1.2', TokenType.FLOAT)
        TEST_NEXT_TOKEN('1e+20', TokenType.FLOAT)
        TEST_NEXT_TOKEN('1e-20', TokenType.FLOAT)
        TEST_NEXT_TOKEN('+1.2e+20', TokenType.FLOAT)
        TEST_NEXT_TOKEN('-1.2e-20', TokenType.FLOAT)
        TEST_NEXT_TOKEN('1/2', TokenType.RATIONAL)
    end)
end)
